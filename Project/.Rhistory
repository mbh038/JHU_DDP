accuracy_mod2
accuracy_mod3
install.packages("gbm")
install.packages("AppliedPredictiveModeling")
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
install.packages("e1071")
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/shiny-examples/063-superzip-example')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
setwd("H:/Rspace/Debs")
## Data from
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
g<-ggplot(pay,aes(x=mfAcademicPayRatio,fill=Type,color=brewer.pal(8,"Set3")))+geom_bar(binwidth=0.02,alpha=0.8)+
#coord_flip()+
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
#scale_fill_gradient("Ratio", low = "green", high = "red")+
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
library(RColorBrewer)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
scale_fill_brewer()+
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
source('H:/Rspace/Debs/HESA-Pay.R', echo=TRUE)
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
shiny::runApp('H:/Rspace/JHU_Data_Science/JHU_DDP/Project')
